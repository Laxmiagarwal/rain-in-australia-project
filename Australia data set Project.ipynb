{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68a9a92",
   "metadata": {},
   "source": [
    "#### Libraries and modules required to run the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing required libraries\n",
    "import opendatasets as od\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import dash\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264567c",
   "metadata": {},
   "source": [
    "# Introduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee067ac8",
   "metadata": {},
   "source": [
    "## Problem statement: \n",
    "\n",
    "To create a fully-automated system that can use today's weather data for a given location to predict whether it will rain at the location tomorrow. This is a binary classification problem.\n",
    "\n",
    "In this project the collected dataset is analysed and i have tried to find out the relationship between the chances of raining the next day and today's various weather indicators such as temperature today, amount of ranifall today, humidity, wind direction, pressure etc. Finally created a Dash app(a web applications) that will tell weather it would rain tomorrow or not based on today's weather data. In this app we can also look at interactive visualizations that would help the users to understand the relationship between the target variables and the predictors. So here the task involves prediction as well as inference.\n",
    "\n",
    "\n",
    "The uses of a web application that can take in today's weather data and predict with high accuracy that whether it would rain tomorrow or not are:\n",
    "\n",
    "1. We can plan outdoor activities, workout or events for the following day, this applications would help us to take informed and data driven decisions.\n",
    "2. Farmers can take decision on irrigiation and plant protection strategies based on the rain prediction.\n",
    "3. Travelers can plan their travel based on the rain predictions.\n",
    "4. Further this could also help in areas such as: Sports event planning, Smart irrigation, Water management, Gardening and Research work and many more.\n",
    "\n",
    "## Dataset used:\n",
    "\n",
    "For this task we train the system on the \"Rain in Australia Dataset\" which is downloaded from kaggle. The dataset was compiled by the Bureau of Meteorology, an Australian government agency responsible for providing weather-related services. The dataset includes various features related to weather conditions such as temperature, humidity, rainfall, wind speed, and more. It also contains the target variable, \"RainTomorrow,\" which indicates whether it rained the next day (Yes/No). \n",
    "\n",
    "RainTomorrow is the target variable to predict. It means -- did it rain the next day, Yes or No? This column is Yes if the rain for that day was 1mm or more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6890eb5",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e933bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.kaggle.com/jsphyg/weather-dataset-rattle-package'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7802b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# {\"username\":\"kaushikthakkar610\",\"key\":\"870c4d33db9f79564265c746eea0ba04\"}\n",
    "od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './weather-dataset-rattle-package'\n",
    "#os.listdir(data_dir)\n",
    "train_csv = data_dir + '/weatherAUS.csv'\n",
    "raw_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613ed37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_df\n",
    "# total number of columns is 23.\n",
    "# total number of rows is 1,45,460."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03669a3a",
   "metadata": {},
   "source": [
    "The dataset contains over 1,45,000 rows and 23 columns. The dataset contains date, numeric and categorical columns. The objective is to create a model to predict the value in the column RainTomorrow. The data set contains the weather information of 49 different locations across australia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91350e4",
   "metadata": {},
   "source": [
    "## Columns in the dataset and their explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87db4a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836a1af",
   "metadata": {},
   "source": [
    "Wind Gust Speed (WindGustSpeed):\n",
    "Wind gust speed represents the maximum wind speed recorded over a short period during a gust of wind. Wind gusts are sudden increases in wind speed that can occur in certain weather conditions, such as during thunderstorms or strong frontal passages. Wind gust speed is usually reported in kilometers per hour (km/h) or meters per second (m/s).\n",
    "\n",
    "Wind Gust Direction (WindGustDir):\n",
    "Wind gust direction indicates the compass direction from which the strongest gusts of wind are blowing. It is reported as a cardinal direction, such as North (N), Northeast (NE), East (E), Southeast (SE), South (S), Southwest (SW), West (W), or Northwest (NW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751a92d",
   "metadata": {},
   "source": [
    "## Classifying the columns into Numeric and Categorical columns:\n",
    "\n",
    "This is essential because different types of columns require different types of data preprocessing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063af4a8",
   "metadata": {},
   "source": [
    "First, We exclude the rows where the value of 'RainTomorrow' or 'RainToday' is missing to make the analysis and make modeling simpler(since one of them is the target variable, and the other is likely to be very closely related to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dropna(subset = ['RainToday', 'RainTomorrow'], inplace=True) # inplace is True that means that changes will be made directly to the new dataframe without creating a new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['Date'] = pd.to_datetime(raw_df['Date'])\n",
    "numeric_cols = raw_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = raw_df.select_dtypes('object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ff586",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numeric_cols), print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9990f31",
   "metadata": {},
   "source": [
    "So now we have extracted the categorical and numerical columns. Now we perform some Exploratory Data anlysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce84d1a",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3fd8ab",
   "metadata": {},
   "source": [
    "Here we perform Exploratory Data Analysis (EDA) which is a critical step in understanding and gaining insights from the dataset before building and training any machine learning models. EDA involves examining and visualizing the data to identify patterns, trends, relationships, and potential issues in the dataset. Through EDA I try to find out Data summary, Univariate Analysis, Bivariate Analysis, Missing value Analysis, Outlier Detection, Time series Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a997ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3153a8",
   "metadata": {},
   "source": [
    "So we see that there is a class imbalance in the final target variable. Approximately there are 3.5 times data with RainTommorr as 'NO' then 'YES'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "px.histogram(raw_df, x = 'Location', title = 'Location vs Rainy Days', \n",
    "             color = 'RainTomorrow')\n",
    "\n",
    "# So in our dataset we have approximately 20% of the times it's \n",
    "# raining tomorrow for almost all locations. It follows more or less a uniform distribution.\n",
    "# so we are not motivated to cosider location as an important factor in our analysis, since it doesn't appear to have an \n",
    "# significant impact on the decision of whether it would rain tomorrow or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. \n",
    "px.histogram(raw_df,\n",
    "            x='Temp3pm',\n",
    "            title='Temperature at 3pm vs. Rain tomorrow',\n",
    "            color='RainTomorrow',\n",
    "            width=850,\n",
    "            height=500)\n",
    "\n",
    "\n",
    "\n",
    "# If we have a moderate temperature at 9am or a bit higher temerature it has more chances to rain.\n",
    "\n",
    "# If low temperature at 3pm, it seems more likely to rain tomorrow. \n",
    "# But there are cases when the temperature is high but it still rains the next day.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(raw_df,\n",
    "            x='Pressure9am',\n",
    "            title='Pressure at 9am vs. Rain tomorrow',\n",
    "            color='RainTomorrow',\n",
    "            width=850,\n",
    "            height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(raw_df,\n",
    "            x='Pressure3pm',\n",
    "            title='Pressure at 3pm vs. Rain tomorrow',\n",
    "            color='RainTomorrow',\n",
    "            width=850,\n",
    "            height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966e4f5",
   "metadata": {},
   "source": [
    "Thus we see that high pressure suggests that it's more likely to rain the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(raw_df.sample(4000),\n",
    "          title = 'Min Temp vs. Max Temp',\n",
    "          x = 'MinTemp',\n",
    "          y = 'MaxTemp',\n",
    "           opacity = 0.7,\n",
    "          color = 'RainTomorrow', width = 850, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08196423",
   "metadata": {},
   "source": [
    "Thus if the variation in Today's temperature is small it's very likely that it would rain tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2df944",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(raw_df.sample(2000),\n",
    "        title = 'Temp (3pm) vs. Humidity (3pm)',\n",
    "        x = 'Temp3pm',\n",
    "        y = 'Humidity3pm',\n",
    "        color = 'RainTomorrow', width = 850, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a004d0e",
   "metadata": {},
   "source": [
    "We can see that if the temperature today is low and humidity is high then there is a fairly good chance of raining tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(raw_df,\n",
    "            x = 'RainTomorrow',\n",
    "            color = \"RainToday\",\n",
    "            title = 'Rain tomorrow vs. Rain Today', width = 850, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c89d88",
   "metadata": {},
   "source": [
    "If it did not rain today then there is a pretty good chance that it won't rain tomorrow. Predicting rain tomorrow 'yes' is difficult than predicting rain tomorrow 'no'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd98767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174aca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(raw_df, x = 'Rainfall', color = 'RainTomorrow')\n",
    "fig.update_xaxes(range=[0, 50]) \n",
    "fig.update_yaxes(range=[0, 1000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb5628",
   "metadata": {},
   "source": [
    "So we see that as the amount of rainfall increases the proportion of days where there is a rain tomorrow increases considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(raw_df.sample(1000),\n",
    "        title = 'Pressure (3pm) vs. Pressure (9am)',\n",
    "        x = 'Pressure3pm',\n",
    "        y = 'Pressure9am',\n",
    "        color = 'RainTomorrow', width = 800, height = 500,\n",
    "        opacity=0.9,\n",
    "        color_discrete_sequence=['#ff7f0e', '#1f77b4'])#ff7f0e\n",
    "\n",
    "fig.update_traces(opacity=0.9, selector=dict(type='scatter', mode='markers', name='Yes'))\n",
    "\n",
    "fig.show()\n",
    "# so we see that if the pressure difference is less we have more chances of raining tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = raw_df[raw_df['RainTomorrow'] == 'Yes']\n",
    "df2 = raw_df[raw_df['RainTomorrow'] == 'No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19107234",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df1, x = 'Rainfall', y = 'Sunshine')\n",
    "\n",
    "# Here we see that comparatively lower sunshine and higher rainfall charactrizes the days for which there is a rainfall tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df2, x = 'Rainfall', y = 'Sunshine')\n",
    "# we see that very lower rainfall and medium sunshine charaterizes the days when it doesn't rains tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9850d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(raw_df.sample(2000),\n",
    "        title = 'Humidity (3pm) vs. Humidity (9am)',\n",
    "        x = 'Humidity3pm',\n",
    "        y = 'Humidity9am',\n",
    "        color = 'RainTomorrow', width = 850, height = 500)\n",
    "\n",
    "# so we see that if the Humidity difference is greater we have more chances of raining tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the change in wind direction affects the chances of raining tomorrow or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3fe4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_wind_direction(row):\n",
    "    if row['WindDir9am'] != row['WindDir3pm']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "raw_df['change_wind_dir'] = raw_df.apply(compare_wind_direction, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['change_wind_dir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(raw_df,\n",
    "            x = 'change_wind_dir',\n",
    "            color = \"RainTomorrow\",\n",
    "            title = '', width = 850, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81311ae8",
   "metadata": {},
   "source": [
    "26% of the times it rains tomorrow when their is no change in the wind direction, 21% of the times it rains tomorrow when their is a change in wind direction, so the change in wind direction does not significantly effect the probability of rain tomorrow.\n",
    "\n",
    "so we drop the above column created column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.drop('change_wind_dir', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571be84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols.remove(\"RainTomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ba1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0b7f8",
   "metadata": {},
   "source": [
    "## Imputing missing Numeric Data:\n",
    "\n",
    "We can't work with the missing values so we need to solve this problem,Now there are several techniques for imputation (filling missing vlaues in the dataset), but I use the most basic one: replacing missing values with the average value in the column using the SimpleImputer class from sklearn.impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8743ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing Missing Numeric Data:\n",
    "\n",
    "# Machine learning models can't work with missing numerical data. The process of filling missing values is called imputation.\n",
    "# Here we replace missing values with the average value in the column using the SimpleImputer class from sklearn.impute.\n",
    "imputer1 = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "\n",
    "\n",
    "# before performing the inputation we check the number of missing values in the data.\n",
    "raw_df[numeric_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60506e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer1.fit(raw_df[numeric_cols])\n",
    "#After calling fit, the computed statistic for each column is stored in the statistics_ property of imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccdaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(imputer1.statistics_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8821f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[numeric_cols] = imputer1.transform(raw_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[numeric_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[categorical_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e2c2e",
   "metadata": {},
   "source": [
    "We Impute the missing values in the categorical columns with the most frequent occuring values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7060a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer2 = SimpleImputer(strategy = 'most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83945bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[categorical_cols] = imputer2.fit_transform(raw_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(imputer2.statistics_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(imputer1.statistics_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[categorical_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457e698",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "\n",
    "Feature Engineering: Feature engineering is the process of creating new features or transforming existing ones in a dataset to enhance the performance of machine learning models. It involves selecting, modifying, or creating features that provide relevant and valuable information to the model, thus improving its ability to make accurate predictions or classifications.\n",
    "\n",
    "Apart form using these columns for prediction I have created new more informative columns that can be used along with these to study the data more effeciently. These are as follows:\n",
    "\n",
    "1. Making a column Temp_diff that captures the Maximum and the Minimum temperature difference in a day.\n",
    "2. Making a column Pressure_diff that calculates the difference in pressure at 3pm and 9am.\n",
    "3. Making a column Humidity_diff that calculates the difference in Humidity at 3pm and 9am.\n",
    "4. Create bins for \"sunshine\" and \"rainfall\" values (e.g., low, medium, high). Combine their categories to make a new feature by merging values or encoding. Convert the merged categories into numerical labels (label or one-hot encoding), add this feature to the dataset, and train your model with it along with other features. Evaluate its impact on model performance using cross-validation or assessing feature importance.\n",
    "\n",
    "\n",
    "\n",
    "Some more ideas that can be implemented are as follows: \n",
    "\n",
    "5. Evaporation, Sunshine: Ratio of evaporation to rainfall, or ratio of sunshine duration to total daylight hours, as they can give information about moisture levels in the atmosphere.\n",
    "6. Wind Features (WindGustDir, WindGustSpeed, WindDir9am, WindDir3pm, WindSpeed9am, WindSpeed3pm): Calculating the difference in wind direction and speed between morning and afternoon, or computing the overall wind speed, as wind patterns can influence rainfall.\n",
    "9. Cloud Features (Cloud9am, Cloud3pm): Difference in cloud cover between morning and afternoon or aggregating cloud cover data to create a new feature.\n",
    "10. Further we can use the wind speed at 3pm and 9am or we can also use the wind gust speed to make a new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "raw_df['Temp_diff'] = abs(raw_df['MaxTemp'] - raw_df['MinTemp'])\n",
    "# px.histogram(raw_df,\n",
    "#             x = 'Temp_diff',\n",
    "#             title = 'Temperature difference during a day vs. Rain tomorrow',\n",
    "#             color = 'RainTomorrow', width = 850, height = 500)\n",
    "numeric_cols.append('Temp_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since by the graph it is seen that the different month, day has more or leass a uniform distribution with respect to RainTomorrow\n",
    "# so we donot include this in our study.\n",
    "\n",
    "# Making a month column and a day of the week column\n",
    "\n",
    "# raw_df['Day_of_week'] = pd.to_datetime(raw_df['Date']).dt.dayofweek\n",
    "# raw_df['Month'] = pd.to_datetime(raw_df['Date']).dt.month\n",
    "# raw_df['week_no'] = pd.to_datetime(raw_df['Date']).dt.isocalendar().week\n",
    "# # raw_df.Month.value_counts()\n",
    "\n",
    "# px.histogram(raw_df,\n",
    "#             x = raw_df.week_no.map(lambda x:str(x)),\n",
    "#             color = raw_df.RainTomorrow)\n",
    "\n",
    "# px.histogram(raw_df,\n",
    "#             x = raw_df.Day_of_week.map(lambda x:str(x)),\n",
    "#             color = raw_df.RainTomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(raw_df,\n",
    "            x = 'Temp_diff',\n",
    "            title = 'Temperature difference during a day vs. Rain tomorrow',\n",
    "            color = 'RainTomorrow', width = 850, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d99cf",
   "metadata": {},
   "source": [
    "In this we see that if the temperature difference duirng a day i.e. the absolute difference between the Maximum adn minimum temperature, is small then we have fairly large chances of raining tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Creating the Pressure difference Column.\n",
    "\n",
    "raw_df['Pressure_diff'] = abs(raw_df['Pressure3pm'] - raw_df['Pressure9am'])\n",
    "numeric_cols.append('Pressure_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(raw_df,\n",
    "            x = 'Pressure_diff',\n",
    "            title = 'Pressure difference during a day vs. Rain tomorrow',\n",
    "            color = 'RainTomorrow', width = 850, height = 500)\n",
    "fig.update_xaxes(range=[0,15]) \n",
    "fig.update_yaxes(range=[0, 4000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab608510",
   "metadata": {},
   "source": [
    "It can be seen that, though not very significant, that low pressure difference is may be factor in determining that it rains tommorrow, through this diagram we can say that low difference may cause rain tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f710a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Creating the Humidity difference Column.\n",
    "\n",
    "raw_df['Humidity_diff'] = abs(raw_df['Humidity3pm'] - raw_df['Humidity9am'])\n",
    "numeric_cols.append('Humidity_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(raw_df,\n",
    "            x = 'Humidity_diff',\n",
    "            title = 'Humidity difference during a day vs. Rain tomorrow',\n",
    "            color = 'RainTomorrow', width = 850, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96549c1c",
   "metadata": {},
   "source": [
    "So we see that low Humidity difference may be a significant factor in determining whether it would rain tomorrow or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e11a6",
   "metadata": {},
   "source": [
    "Now we feature engineer the sunshine and the Rainfall column to create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunshine_bins = [-1,5,10,15] # Low, Medium, High sunshine\n",
    "rainfall_bins = [-1, 50, 250, 400]  # No rain, Low, Medium, High rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20395f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['Sunshine_Category'] = pd.cut(raw_df['Sunshine'], bins=sunshine_bins, labels=['Low', 'Medium', 'High'])\n",
    "raw_df['Rainfall_Category'] = pd.cut(raw_df['Rainfall'], bins=rainfall_bins, labels=['Low', 'Medium', 'High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['Combined_Feature'] = raw_df['Sunshine_Category'].astype(str) + '_' + raw_df['Rainfall_Category'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3272f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categories to numerical labels\n",
    "raw_df['Combined_Encoded'] = raw_df['Combined_Feature'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a093438",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['Combined_Encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.drop('Combined_Encoded', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aeae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab4d43",
   "metadata": {},
   "source": [
    "So it seems that the samples that are coded with 2,4,6 that is Low_High, Low_Medium, Medium_Medium, where the first word denotes sunshine and the second word denotes the Rain category, has almost for all cases that it rains tomorrow. Though the number of such instances are very less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52215e2d",
   "metadata": {},
   "source": [
    "## Encoding Categorical columns:\n",
    "\n",
    "Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d12221",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = raw_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = raw_df.select_dtypes('object').columns.tolist()\n",
    "print(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4651055",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols.remove(\"RainTomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb11787",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoder.fit(raw_df[categorical_cols])\n",
    "encoder.categories_\n",
    "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n",
    "len(encoded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[encoded_cols] = encoder.transform(raw_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25246d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = numeric_cols + encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[required_cols].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c7e74",
   "metadata": {},
   "source": [
    "## Creating Training, Validation and Test Splits in the dataset.\n",
    "\n",
    "As a general rule of thumb we can use around 60% of the data for the training set, 20% for the validation set and 20% for the test set. When rows in the dataset have no inherent order, it's common practice to pick random subsets of rows for creating test and validation sets. \n",
    "\n",
    "But in this case we since we are workign with dates it's a better idea to separate the train, validation and test datasets according to time so that the model is trained on the past data and is evaluated on the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if we select rows randomly set the rand to True\n",
    "rand = False\n",
    "if rand:\n",
    "    train_val_df, test_df = train_test_split(raw_df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d774130",
   "metadata": {},
   "source": [
    "For the current dataset, I have used the Date column in the dataset to create another column for year. I picked the last two years for the test set, and one year before it for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a60a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('No. of Rows per Year')\n",
    "sns.countplot(x=pd.to_datetime(raw_df.Date).dt.year);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = raw_df['Date'].dt.year\n",
    "train_df = raw_df[year < 2015] # we will train the model on the data before 2015\n",
    "val_df = raw_df[year == 2015]  # validation set would consist of data of 2015\n",
    "test_df = raw_df[year > 2015]  # test set would consist of data after 2015\n",
    "print('train_df.shape :', train_df.shape)\n",
    "print('val_df.shape :', val_df.shape)\n",
    "print('test_df.shape :', test_df.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75992c34",
   "metadata": {},
   "source": [
    "We have also ensured that the train validation and test sets all contain data for all 12 months of the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db86abb",
   "metadata": {},
   "source": [
    "## Identifying the Input and Target Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aaca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = numeric_cols + encoded_cols\n",
    "target_col = 'RainTomorrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_df[input_cols].copy()\n",
    "train_targets = train_df[target_col].copy()\n",
    "\n",
    "val_inputs = val_df[input_cols].copy()\n",
    "val_targets = val_df[target_col].copy()\n",
    "\n",
    "test_inputs = test_df[input_cols].copy()\n",
    "test_targets = test_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have only 23 percent of the data points for which it is raining tomorrow, this displays a class imbalance in \n",
    "# in the training data.\n",
    "train_targets.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684987a",
   "metadata": {},
   "source": [
    "## Scaling Numeric Features\n",
    "\n",
    "This is required because:\n",
    "\n",
    "1. Different features may have different scales, which can lead to numerical instability in many algorithms, Scaling ensures all the features are on the similar scale and no feature dominates the learning process just due to it's larger values.\n",
    "2. Scaling further speeds up the Convergence of optimization algorithms, allowing them to reach the optimal solution more quickly and efficiently.\n",
    "3. It enhances the performance of alogorithms that depend on distance based calculations or gradients.\n",
    "4. Some algorithms, like linear regression, assess feature importance based on the magnitude of their coefficients. Scaling ensures that features with smaller numerical values are not overlooked when calculating importance.\n",
    "5. Regularization techniques, like L1 and L2 regularization, are applied to penalize large coefficients in linear models. Scaling ensures that all features are penalized equally, regardless of their original scales.\n",
    "6. Algorithms that rely on distance measures, such as clustering algorithms, are sensitive to feature scales. Scaling ensures that the distances are calculated correctly based on the actual significance of the features.\n",
    "7. It's important to note that some algorithms, like tree-based models (Random Forest, Gradient Boosting), are less sensitive to feature scales due to their internal structures. However, in many cases, scaling still provides benefits and good practices for consistent performance across different types of algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cbdaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc22262",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(raw_df[numeric_cols])\n",
    "print(list(scaler.data_min_), list(scaler.data_max_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23786ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ae927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ab73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01c3ab",
   "metadata": {},
   "source": [
    "## Training a Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e128f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(numeric_cols + encoded_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec733dc2",
   "metadata": {},
   "source": [
    "Before we had 97988 rows but after resampling we had now 152380 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28812aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e595c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_.tolist(), model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1e644",
   "metadata": {},
   "source": [
    "## Making Prediction and Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a46843",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_inputs[numeric_cols + encoded_cols]\n",
    "X_val = val_inputs[numeric_cols + encoded_cols]\n",
    "X_test = test_inputs[numeric_cols + encoded_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ad7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa35e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can output a probabilistic prediction using predict_proba.\n",
    "train_probs = model.predict_proba(X_train)\n",
    "train_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bd5f7",
   "metadata": {},
   "source": [
    "We can test the accuracy of the model's predictions by computing the percentage of matching values in train_preds and train_targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198347f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train_targets, train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634671da",
   "metadata": {},
   "source": [
    "Hence the Logistics Regression Model gives an accuracy of 85.25% on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11246362",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d94ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds, val_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(val_preds, val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a885b",
   "metadata": {},
   "source": [
    "The logistics Regression model gives an accuracy of 85.48% on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74470b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "accuracy_score(test_targets, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220a453",
   "metadata": {},
   "source": [
    "The Logistics Regression model gives an accuracy of 84.22% on the Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_targets, train_preds, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fcc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a helper function to generate predictions, compute the accuracy score, and plot a confusion matrix for a given \n",
    "# set of inputs.\n",
    "\n",
    "def predict_and_plot(inputs, targets, name=''):\n",
    "    preds = model.predict(inputs)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    \n",
    "    cf = confusion_matrix(targets, preds, normalize='true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('{} Confusion Matrix'.format(name));\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = predict_and_plot(X_train, train_targets, 'Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeafa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = predict_and_plot(X_val, val_targets, 'Validatiaon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = predict_and_plot(X_test, test_targets, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how good is the accuracy of 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_guess(inputs):\n",
    "    return np.random.choice([\"No\", \"Yes\"], len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_targets, random_guess(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cedb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_no(inputs):\n",
    "    return np.full(len(inputs), \"No\")\n",
    "accuracy_score(test_targets, all_no(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd15b1",
   "metadata": {},
   "source": [
    "Our random model achieves an accuracy of 50% and our \"always No\" model achieves an accuracy of 77%. \n",
    "\n",
    "Thus, our model is better than a \"dumb\" or \"random\" model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2ffe8",
   "metadata": {},
   "source": [
    "### Making prediction on a new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9065d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = {'Date': '2021-06-19',\n",
    "             'Location': 'Katherine',\n",
    "             'MinTemp': 23.2,\n",
    "             'MaxTemp': 33.2,\n",
    "             'Rainfall': 10.2,\n",
    "             'Evaporation': 4.2,\n",
    "             'Sunshine': np.nan,\n",
    "             'WindGustDir': 'NNW',\n",
    "             'WindGustSpeed': 52.0,\n",
    "             'WindDir9am': 'NW',\n",
    "             'WindDir3pm': 'NNE',\n",
    "             'WindSpeed9am': 13.0,\n",
    "             'WindSpeed3pm': 20.0,\n",
    "             'Humidity9am': 89.0,\n",
    "             'Humidity3pm': 58.0,\n",
    "             'Pressure9am': 1004.8,\n",
    "             'Pressure3pm': 1001.5,\n",
    "             'Cloud9am': 8.0,\n",
    "             'Cloud3pm': 5.0,\n",
    "             'Temp9am': 25.7,\n",
    "             'Temp3pm': 33.0,\n",
    "             'RainToday': 'Yes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86fd408",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_df = pd.DataFrame([new_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43512e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = new_input_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = new_input_df.select_dtypes('object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols.remove(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6dbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_df[numeric_cols] = imputer1.transform(new_input_df[numeric_cols])\n",
    "new_input_df[categorical_cols] = imputer2.transform(new_input_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_df['Temp_diff'] = abs(new_input_df['Temp3pm']-new_input_df['Temp9am'])\n",
    "new_input_df['Pressure_diff'] = abs(new_input_df['Pressure3pm']-new_input_df['Pressure9am'])\n",
    "new_input_df['Humidity_diff'] = abs(new_input_df['Humidity3pm']-new_input_df['Humidity9am'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535eb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_df['Sunshine_Category'] = pd.cut(new_input_df['Sunshine'], bins=sunshine_bins, labels=['Low', 'Medium', 'High'])\n",
    "new_input_df['Rainfall_Category'] = pd.cut(new_input_df['Rainfall'], bins=rainfall_bins, labels=['Low', 'Medium', 'High'])\n",
    "new_input_df['Combined_Feature'] = new_input_df['Sunshine_Category'].astype(str) + '_' + new_input_df['Rainfall_Category'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = new_input_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = new_input_df.select_dtypes('object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c968294",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols.remove('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fa45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_df[numeric_cols] = scaler.transform(new_input_df[numeric_cols])\n",
    "new_input_df[encoded_cols] = encoder.transform(new_input_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_input = new_input_df[numeric_cols + encoded_cols]\n",
    "X_new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_new_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2899366",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99dafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_input(single_input):\n",
    "#     input_df = pd.DataFrame([single_input])\n",
    "#     input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n",
    "#     input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n",
    "#     input_df[encoded_cols] = encoder.transform(input_df[categorical_cols])\n",
    "#     X_input = input_df[numeric_cols + encoded_cols]\n",
    "#     pred = model.predict(X_input)[0]\n",
    "#     prob = model.predict_proba(X_input)[0][list(model.classes_).index(pred)]\n",
    "#     return pred, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69242107",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539044d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import jovian\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_inputs[numeric_cols + encoded_cols]\n",
    "X_val = val_inputs[numeric_cols + encoded_cols]\n",
    "X_test = test_inputs[numeric_cols + encoded_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b56081",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03411ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dadc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1794556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train_targets, train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c5749",
   "metadata": {},
   "source": [
    "The training set accuracy is 100%, but we are not interested in this, the area of interest is to see how well does the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa98d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(X_val)\n",
    "accuracy_score(val_targets, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01852ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "accuracy_score(test_targets, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fae8a8",
   "metadata": {},
   "source": [
    "It seems that apart from achieving 100% accuracy on the training set, the validation and the test accuracy is pretty low, this indicates that the model has memorized the training examples and do not generalize well to the unseen data. This is overfitting or we can say that the model has has variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668520ff",
   "metadata": {},
   "source": [
    "#### Creating some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68398629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree, export_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model, feature_names=X_train.columns, max_depth=2, filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2d8ee",
   "metadata": {},
   "source": [
    "##### Feature Importance\n",
    "\n",
    "Based on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb63495",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Feature Importance')\n",
    "sns.barplot(data=importance_df.head(10), x='importance', y='feature');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9626b6",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning and Reducing Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1710db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_depth_error(md):\n",
    "    model = DecisionTreeClassifier(max_depth=md, random_state=42)\n",
    "    model.fit(X_train, train_targets)\n",
    "    train_acc = 1 - model.score(X_train, train_targets)\n",
    "    val_acc = 1 - model.score(X_val, val_targets)\n",
    "    return {'Max Depth': md, 'Training Error': train_acc, 'Validation Error': val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1924d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "errors_df = pd.DataFrame([max_depth_error(md) for md in range(1, 21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df.sort_values('Validation Error', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e583d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(errors_df['Max Depth'], errors_df['Training Error'])\n",
    "plt.plot(errors_df['Max Depth'], errors_df['Validation Error'])\n",
    "plt.title('Training vs. Validation Error')\n",
    "plt.xticks(range(0,21, 2))\n",
    "plt.xlabel('Max. Depth')\n",
    "plt.ylabel('Prediction Error (1 - Accuracy)')\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef88034",
   "metadata": {},
   "source": [
    "Thus from the diagram it is that the max_depth of 8 gives a balanced train_accuracy as well as validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32539e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=8, random_state=42).fit(X_train, train_targets)\n",
    "model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716a447",
   "metadata": {},
   "source": [
    "#### Max_leaf_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6747c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_leaf_nodes=128, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f820a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding a combination of max_depth and max_leaf_nodes that gives least validation set error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_leaf(depth_list, max_leaf_list):\n",
    "    combination = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    \n",
    "    for i in depth_list:\n",
    "        for j in max_leaf_list:\n",
    "            combination.append([i, j])\n",
    "            model = DecisionTreeClassifier(max_depth=i, max_leaf_nodes=j, random_state=42)\n",
    "            model.fit(X_train, train_targets)\n",
    "            train_acc = model.score(X_train, train_targets)\n",
    "            val_acc = model.score(X_val, val_targets)\n",
    "            train_acc_list.append(train_acc)\n",
    "            val_acc_list.append(val_acc)\n",
    "            \n",
    "    data = {'Combination': combination, 'Training_accuracy': train_acc_list, 'Validation_accuracy': val_acc_list}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967757d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = depth_leaf([i for i in range(5,11)], [i for i in range(100,130)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb.sort_values('Validation_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64351c69",
   "metadata": {},
   "source": [
    "Thus the decision tree with max_depth of 9 and maximum leaf nodes of 120 gives the highest validation accuracy of 84.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da55a7f",
   "metadata": {},
   "source": [
    "### Training a random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532283f1",
   "metadata": {},
   "source": [
    "The random forest model is a model in which we combine the results of several decision trees trained with slightly different parameters. The idea is that each decision tree in the forest would make some kind of errors and upon averaging, many of these errors will cancel out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52888223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0740533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14e505",
   "metadata": {},
   "source": [
    "`n_jobs` allows the random forest to use mutiple parallel workers to train decision trees, and `random_state=42` ensures that the we get the same results for each execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3352e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs = model.predict_proba(X_train)\n",
    "train_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3f222",
   "metadata": {},
   "source": [
    "Looking at individual decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3557c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model.estimators_[0], max_depth=2, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152708eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model.estimators_[20], max_depth=2, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd848985",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de065ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb94f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Feature Importance')\n",
    "sns.barplot(data=importance_df.head(10), x='importance', y='feature');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa449334",
   "metadata": {},
   "source": [
    "An important insight about the usefullness of ensembling technique is that the distribution of important features is much less skewed now than it was before when we were fitting a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774518f",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = RandomForestClassifier(random_state=42, n_jobs=-1).fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_acc = base_model.score(X_train, train_targets)\n",
    "base_val_acc = base_model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a88f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs = base_train_acc, base_val_acc\n",
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e360f",
   "metadata": {},
   "source": [
    "### Some more hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68038097",
   "metadata": {},
   "source": [
    "#### n_estimators : \n",
    "This controls the number of decision trees in the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fe643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train, train_targets)\n",
    "model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b63a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=500)\n",
    "model.fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03452a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=250)\n",
    "model.fit(X_train, train_targets)\n",
    "model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794800a",
   "metadata": {},
   "source": [
    "#### Max Leaf Nodes and Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54369bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a helper function to test hyperparameters\n",
    "def test_params(**params):\n",
    "    model = RandomForestClassifier(random_state=42, n_jobs=-1, **params).fit(X_train, train_targets)\n",
    "    return model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_depth=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_leaf_nodes=2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60f29c",
   "metadata": {},
   "source": [
    "Let's put the value of previously obtained max_leaf_nodes and max_max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_leaf_nodes = 250, max_depth = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5a5fd",
   "metadata": {},
   "source": [
    "The accuracy seems to be low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff16f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  max_features: default value is sqrt(n), that means:\n",
    "#  only sqrt(n) out of total features (n) to be chosen randomly at each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e479342",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_features='log2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bdd70",
   "metadata": {},
   "source": [
    "### `min_samples_split` and `min_samples_leaf`\n",
    "\n",
    "By default, the decision tree classifier tries to split every node that has 2 or more. You can increase the values of these arguments to change this behavior and reduce overfitting, especially for very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043defdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(min_samples_split=100, min_samples_leaf=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(min_samples_split=50, min_samples_leaf=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8705a72",
   "metadata": {},
   "source": [
    "### `min_impurity_decrease`\n",
    "\n",
    "This argument is used to control the threshold for splitting nodes. A node will be split if this split induces a decrease of the impurity (Gini index) greater than or equal to this value. It's default value is 0, and you can increase it to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13000f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(min_impurity_decrease=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebddfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283054fd",
   "metadata": {},
   "source": [
    "#### `bootstrap`, `max_samples` \n",
    "\n",
    "By default, a random forest doesn't use the entire dataset for training each decision tree. Instead it applies a technique called bootstrapping. For each tree, rows from the dataset are picked one by one randomly, with replacement i.e. some rows may not show up at all, while some rows may show up multiple times.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/W8UGaEA.png\" width=\"640\">\n",
    "\n",
    "Bootstrapping helps the random forest generalize better, because each decision tree only sees a fraction of th training set, and some rows randomly get higher weightage than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a688d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_samples=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0aea34",
   "metadata": {},
   "source": [
    "#### class_weights\n",
    "\n",
    "The purpose of using class weights is to address class imbalance in the training data. When one class has significantly more samples than the other, it might lead the model to be biased towards the majority class. Assigning higher weights to the minority class helps the model pay more attention to it and prevents it from being overshadowed by the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(class_weight={'No': 1, 'Yes': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283554dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so finally we have:\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, \n",
    "                               random_state=42, \n",
    "                               n_estimators=500,\n",
    "                               max_features=7,\n",
    "                               max_depth=30, \n",
    "                               class_weight={'No': 1, 'Yes': 1.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, train_targets), model.score(X_val, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec729f",
   "metadata": {},
   "source": [
    "## SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28313807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize and train an SVM classifier\n",
    "svm_classifier = SVC(kernel=\"linear\")  # You can experiment with different kernels\n",
    "svm_classifier.fit(X_train, train_targets)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_targets, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "classification_rep = classification_report(test_targets, predictions)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize and train an SVM classifier\n",
    "svm_classifier = SVC(kernel=\"rbf\")  # You can experiment with different kernels\n",
    "svm_classifier.fit(X_train, train_targets)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_targets, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "classification_rep = classification_report(test_targets, predictions)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e06f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize and train an SVM classifier\n",
    "svm_classifier = SVC(kernel=\"poly\")  # You can experiment with different kernels\n",
    "svm_classifier.fit(X_train, train_targets)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_targets, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "classification_rep = classification_report(test_targets, predictions)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943a1e8",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee057842",
   "metadata": {},
   "source": [
    "For all the models applied in the study, It was found that Random Forest gives the best validation set accuracy i.e. 85.95% Hence we finally select that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef1ae7",
   "metadata": {},
   "source": [
    "## Future work ideas:\n",
    "\n",
    "Since this data is a Spatio-Temporal data i.e. it includes both Spatial and Time component, so Analyzing these kind of data often involves techniques like space-time clustering, geostatistics, spatial interpolation, times series analysis, and various machine learning and data mining techinques that are tailored for such spatio-temporal patterns. Further I believe this would also require some doamin specific knowledge of geography and rainfall related sciences.\n",
    "\n",
    "Studying this data as a spatio-temporal data can provide insights into dynamic processes that occur across both space and time, enabling more accurate predictions, better decision-making, and a deeper understanding of complex phenomena.\n",
    "\n",
    "Examples of other models that could be used are: Long Short-Term Memory (LSTM) Networks, Convolutional Neural Networks (CNNs), Kriging, Spatial Regression(Geographically Weighted Regression (GWR) and Spatial Autoregressive Models (SAR) fall under this category) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43337ad7",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe363a18",
   "metadata": {},
   "source": [
    "Books:\n",
    "1. ISLR\n",
    "2. Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85a074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
